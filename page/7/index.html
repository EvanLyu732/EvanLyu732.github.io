<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


    
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://evanlyu732.github.io/atom.xml">
    
    
    
        
    
    
    
    
    
    
        
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>evan lyu</title>
    <meta charset="utf-8">
    <meta name="title" content="evan lyu">
    <meta name="description" content="">
    <meta property="og:image:width" content="200" />
    <meta property="og:image:height" content="200" />
    <link rel="stylesheet" href="https:&#x2F;&#x2F;evanlyu732.github.io/style.css">
    
    
    <style>
    @media screen and (min-width: 320px) {
        body {
            font-size: calc(16px + 2 * ((100vw - 320px) / 960));
        }
    }
    </style>
    
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Source+Serif+Pro:wght@400;700&display=swap">
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:wght@400;700&display=swap">
    <style>body { font-family: 'Source Serif Pro', 'Source Han Serif SC', 'Noto Serif CJK SC', 'Noto Serif SC', serif }</style>
    
</head>
<body>
    
    <header class="header">
        <div class="blog-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io" class="logo">evan lyu</a></div>
        <nav class="navbar">
    <ul class="menu">
        
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;" class="current-menu-item-link">主页</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;archives" class="menu-item-link">归档</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;about" class="menu-item-link">关于</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;note" class="menu-item-link">书架</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;atom.xml" class="menu-item-link">订阅</a>
            
        </li>
        
    </ul>
</nav>

    </header>
    
    <main class="main">
        
<section class="posts">
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog10&#x2F;" class="post-title-link">五分钟读论文: Scaling and evaluating sparse autoencoders</a></div>
        <div class="post-content">
            
                <p>这几天OpenAI新发了一篇<a href="https://openai.com/index/extracting-concepts-from-gpt-4/">博客</a>关于使用sparse-autoencoders去解释GPT4的内部机制. 本文是对论文的解读方便读者快速获取关键信息, 如果好求甚解, 请点击传送门:</p>
<ul>
<li>blog: <a href="https://openai.com/index/extracting-concepts-from-gpt-4/">Extracting Concepts from GPT-4</a></li>
<li>paper: <a href="https://cdn.openai.com/papers/sparse-autoencoders.pdf">Scaling and evaluating sparse autoencoders</a></li>
<li>code: <a href="https://github.com/openai/sparse_autoencoder">openai/sparse_autoencoder</a></li>
</ul>
<p><a id="目录"></a></p>
<h1 id="mu-lu">目录</h1>
<ul>
<li><a href="https://evanlyu732.github.io/blog10/#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86">背景知识</a>
<ul>
<li><a href="https://evanlyu732.github.io/blog10/#autoencoder">Autoencoder</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#sparse-autoencoder">Sparse Autoencoder</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#sparse-autoencoder%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7">Sparse Autoencoder的特征可解释性</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#bottleneck-layer">Bottleneck Layer</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#topk%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">TopK激活函数</a></li>
</ul>
</li>
<li><a href="https://evanlyu732.github.io/blog10/#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB">论文解读</a></li>
<li><a href="https://evanlyu732.github.io/blog10/#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>
<p>如果对背景知识不太了解的话, 推荐顺序阅读. 如果已经有足够的背景知识, 请点击<a href="https://evanlyu732.github.io/blog10/#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB">这里</a>开始正文阅读. 下面是对论文的解读. </p>
<p><a id="背景知识"></a></p>
<h1 id="bei-jing-zhi-shi">背景知识</h1>
<p><a id="Autoencoder"></a></p>
<h2 id="autoencoder">Autoencoder</h2>
<p>根据吴恩达在CS294A的<a href="http://stanford.edu/class/cs294a/sparseAutoencoder.pdf">讲义</a>, Autoencoder是一种无监督学习, 在没有给定标签的数据{x(1), x(2), x(3)....}, 通过隐藏层(hidden layer)学习输入中隐含的特征, 从而让输出{x^(1), x^(2), x^(3)...}尽可能的逼近输入. 下图为autoencoder的结构(引用自<a href="http://stanford.edu/class/cs294a/sparseAutoencoder.pdf">这里</a>):</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/autoencoder.png" height="100" width="100"/>
<!-- ![autoencoder](https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/autoencoder.png) -->
<p><a id="Sparse Autoencoder"></a></p>
<h2 id="sparse-autoencoder">Sparse Autoencoder</h2>
<p>Sparse Autoencoder是Autoencoder的一种变种, 在Autoencoder的基础上通过增加稀疏惩罚项(sparse penalty)使得前向传播时只有一部分神经元激活, 而不是所有神经元都激活. 由于相比与未做稀疏化处理的autoencoder结构, 激活的隐藏层神经元(hidden neruon)变少了, 因此每一个隐藏层神经元都包含了更丰富的隐藏特征(latent feature). 是不是和现在流行的<a href="https://huggingface.co/blog/moe">Mixture of Experts</a>很像? 
下图为sparse autoencoder的结构(引用自<a href="https://medium.com/@syoya/what-happens-in-sparse-autencoder-b9a5a69da5c6">这里</a>):</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/sparse-autoencoder2.png" alt="sparse-autoencoder" /></p>
<p><a id="sparse-autoencoder的特征可解释性"></a></p>
<h2 id="sparse-autoencoderde-te-zheng-ke-jie-shi-xing">Sparse Autoencoder的特征可解释性</h2>
<p>MIT6.S898 Deep Learning在2023年发表的一篇<a href="https://deep-learning-mit.github.io/staging/blog/2023/learning-interpretable-features-with-sparse-autoencoders/">blog</a>在这里提到了</p>
<blockquote>
<p>A sparse autoencoder lets us learn a sparse representation for a vector, but in a higher dimensional space.</p>
</blockquote>
<p>相比与未稀疏化的Autoencoder, 稀疏化的Autoencoder可以学习到更高维度的隐含特征. 换个角度说, 未经稀疏化的隐藏层神经往往是表示多维特征(polysemantic). 而经过稀疏化的隐藏层神经元所表示的特征维度更少, 从而使得隐含特征更加容易理解. 更详细的关于sparse autoencoder的可解释性, 可以点击Anthropic发的<a href="https://transformer-circuits.pub/2023/monosemantic-features">&quot;Towards Monosemanticity: Decomposing Language Models With Dictionary Learning&quot;</a>查看.</p>
<p><a id="Bottleneck Layer"></a></p>
<h2 id="bottleneck-layer">Bottleneck Layer</h2>
<p>bottlenect layer指的是含有比前一层更少的神经元的网络层, 使得输入特征维度减少. 这里引用英文原文更方便理解:</p>
<blockquote>
<p>A bottleneck layer is a layer that contains few nodes compared to the previous layers. It can be used to obtain a representation of the input with reduced dimensionality.</p>
</blockquote>
<p><a id="TopK激活函数"></a></p>
<h2 id="topkji-huo-han-shu">TopK激活函数</h2>
<p>TopK是一种激活函数. 仅保留输入向量中最大的k的值，其余值设置为0.</p>
<p><a id="论文解读"></a></p>
<h1 id="lun-wen-jie-du">论文解读</h1>
<p>ok, 我们已经了解了所有的前置知识, 接下来我们开始看这篇文章. 首先是作者部分还有Ilya Sutskever与Jan Leike, 说明是OpenAI之前研究的存货. 再来看摘要部分:</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/openai-papers.png" alt="paper-abstract" /></p>
<p>黄色部分的文字是这篇文章所解决的问题, 绿色部分是作者提出的解决方案. </p>
<ul>
<li><strong>问题</strong> : 随着输入特征的增加, 训练sparse autoencoder会难以平衡稀疏性与准确性. 稀疏性指的是如何确定哪些神经元需要激活, 哪些神经元不需要激活. 准确性是指经过稀疏化处理后的隐藏层神经元所表示的隐含特征是否与原始输入特征相似(见背景知识Autoencoder). </li>
<li><strong>解决方案</strong> : 作者使用k-sparse autoencoder去控制稀疏性从而实现平衡. </li>
</ul>
<p>到了正文部分, 作者一开始比较了不同的激活函数对autoencoder的影响. 发现TopK的获得最小的正规化均方根误差(Normalized root mean square error). 如下图所示:</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/openai-topk.png" alt="topk" /></p>
<p>接下来作者在不同指标上又进行了测试, 如果有兴趣建议阅读原文. 为了抓住重点, 我们先跳过这一部分. 作者使用<a href="https://arxiv.org/pdf/2305.19911">Neuron to Graph(N2G)</a>去做特征的解释.</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/n2g.png" alt="n2g" /></p>
<p>上图的N2G论文原文的描述, 可以看到N2G是将语言模型输出的回答进行关联生成一张有向图. </p>
<p>之后作者对比了ReLU与TopK在N2G中的表现. 发现TopK的召回率以及精度都更高. 如下图所示:</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/openai-result.png" alt="result" /></p>
<p>最后作者给出了结论以及未来工作的方向:</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/paper-result.png" alt="paper-result" /></p>
<p><a id="参考资料"></a></p>
<h1 id="can-kao-zi-liao">参考资料</h1>
<ul>
<li><a href="http://stanford.edu/class/cs294a/sparseAutoencoder.pdf">Sparse autoencoder, CS294A Lecture notes - Andrew Ng</a></li>
<li><a href="https://stats.stackexchange.com/questions/262044/what-does-a-bottleneck-layer-mean-in-neural-networks">What does a bottleneck layer mean in neural networks?</a></li>
</ul>
<hr />
<br>
<br>
<p>(第一次写这种论文解读类文章, 发现还是不好写. 因为有条件的话应该直接读原文, 不加如一些原文的图的话会表达不到意思, 加了太多又和读原文没什么区别.)</p>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年六月七日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog9&#x2F;" class="post-title-link">Nvidia CUDA的前世今生</a></div>
        <div class="post-content">
            
                <blockquote>
<p>Any sufficiently advanced technology is indistinguishable from magic. - Arthur C. Clarke</p>
</blockquote>
<p>CUDA(Compute Unified Device Architecture)是Nvidia强大的护城河之一, 也绝对算是科技黑魔法.</p>
<p>最近突然冒出了一个想法, 为什么CUDA要这样设计(kernel, warp, thread, block, grid...)? 带着这个疑问, 我花了一些时间
在调研CUDA背后的历史, 以下是对Nvidia重要护城河之一的CUDA为什么这样设计的介绍:</p>
<h2 id="cudafa-zhan-li-shi">CUDA发展历史</h2>
<p>要了解CUDA为什么这样设计就需要了解CUDA的发展时间线. 在维基百科上搜索<a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a>, 里面有提到CUDA的发展历史, 以下为<a href="https://en.wikipedia.org/wiki/CUDA#:~:text=Ian%20Buck%2C%20while,neural%20networks.%5B8%5D">原文</a>:</p>
<blockquote>
<p><strong>Ian Buck</strong>, while at <strong>Stanford</strong> in 2000, created an 8K gaming rig using 32 GeForce cards, then obtained a DARPA grant to perform <strong>general purpose parallel programming on GPUs</strong>. He then joined Nvidia, where since 2004 he has been overseeing CUDA development. In pushing for CUDA, Jensen Huang aimed for the Nvidia GPUs to become a <strong>general hardware</strong> for scientific computing. CUDA was released in 2006. Around 2015, the focus of CUDA changed to neural networks.</p>
</blockquote>
<p>谁是Ian Buck? 继续搜索Ian Buck会发现他现在英伟达的<a href="https://blogs.nvidia.com/blog/author/ian-buck/">副总裁</a>, 以前是斯坦福的博士生, 点击Ian博士时期的<a href="http://scroll.stanford.edu/~ianbuck/">主页</a>. 可以看到他在博士期间做的项目<a href="http://graphics.stanford.edu/projects/brookgpu/">BrookGPU</a>, 里面有提到Brook是Ian提出的针对与GPU编程的流式编程语言. 此外在CUDA1.0时期的<a href="https://www.youtube.com/watch?v=Cmh1EHXjJsk">采访</a>，Ian在视频里面明确提到将<strong>Brook的想法迁移到CUDA</strong>上, 因此可以认为<strong>Brook就是CUDA最早的雏形</strong>. </p>
<h2 id="ian-buckde-brookyu-yan">Ian Buck的Brook语言</h2>
<p>让我们一起再来看看BrookGPU的项目介绍:</p>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/brookgpu.png" alt="brook-gpu" /></p>
<p>总结一下, Brook提出了<strong>stream programming model</strong>以及<strong>stream application</strong>并给出了对应的实现去解决<a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86%E5%99%A8%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97"><strong>通用计算</strong></a>的问题.</p>
<p>再结合BrookGPU当时的论文<a href="https://graphics.stanford.edu/papers/brookgpu/brookgpu.pdf">Brook for GPUs: Stream Computing on Graphics Hardware</a>以及Ian Buck的博士论文<a href="http://graphics.stanford.edu/~ianbuck/thesis.pdf">STREAM COMPUTING ON GRAPHICS HARDWARE</a>. 我们能够了解到以下几个点:</p>
<ul>
<li>由于GPU与CPU所针对的场景不同，并且由于CPU的局限性: 有限的instruction level parallelism (ILP), 过度使用缓存去获取局部性. 因此GPU的编程语言形态会与CPU有所不同.</li>
<li>当时也有针对于GPU编程的语言, 但是这些语言的实现都是针对于图形学的shader所设计的, 而不是为了<strong>通用计算编程</strong>所设计</li>
<li>程序在GPU上执行时，其内存访问延迟主要受到两个关键因素的制约: 一是计算速率(compute rate), 二是内存带宽(memory bandwidth). 这意味着，GPU处理数据的速度和内存传输数据的能力共同决定了程序在内存操作中的延迟表现. BrookGPU的设计是为了让GPU计算充分发挥, 从而让内存时延主要部分为内存带宽.</li>
<li>Brook不是第一个尝试去解决通用计算问题, 但是是第一个使用流式处理(stream processing)去解决通用计算. </li>
</ul>
<p><img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/brookgpu2.png" alt="advantage-of-streaming" /></p>
<p>在Ian提出Streaming Processing框架的这一节提到了很重要的一个观点. 内存的访问速度(memory access rate)即内存带宽(memory bandwidth)往往
是制约程序性能的瓶颈. 而软件层由于改变不了物理的内存带宽, 因而会采取其他技巧去隐藏内存延迟(memory latency). 这里Streaming Processing使用data parallelism与<a href="https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/introduction/">arithmetic Intensity</a>去隐藏内存延迟(hide memory latency). Ian也提出了什么是arithmetic Intensity的概念:</p>
<blockquote>
<p>In order to quantify this property, we introduce the concept of arithmetic inten-
sity. <strong>Arithmetic intensity is the ratio of arithmetic operations performed per memory
operation, in other words, flops per word transferred.</strong></p>
</blockquote>
<p>感觉这里解释的不是很清楚, 于是询问GPT4的回答, 讲的很清楚:
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/arth-intensity.png" alt="arithmetic Intensity" />
Arithmetic intensity指的是运算指令(add, mul..)与内存操作(load, store..)的比例.</p>
<p>这里小结一下Brook做了什么事:</p>
<p><em>Brook是一种编程语言, 通过软件层的抽象来让程序尽可能得调用GPU的运算单元，尽可能减少内存搬运操作. 从而隐藏内存延迟.</em></p>
<p>如果有兴趣详细了解的话, GTC2022上的这个<a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41487/">talk</a>会很有帮助. 在了解了这些之后, 我们再来一起看看BrookGPU与CUDA的对比.</p>
<h2 id="brookyu-cudade-dui-bi">Brook与CUDA的对比</h2>
<p>接下来结合上面的信息以及CUDA1.0的<a href="https://developer.download.nvidia.cn/compute/cuda/1.0/NVIDIA_CUDA_Programming_Guide_1.0.pdf">参考手册</a>, 我们来看看Brook与Cuda的一些概念的对比. 由于CUDA是在Brook之后诞生, 所以这里以Brook中的概念为准分各个小节. 为了尽可能得传递原始信息避免加工, 每个概念会附上原文.</p>
<h3 id="stream">Stream</h3>
<blockquote>
<p>A stream is a collection of data which can be operated on in parallel.  - Ian Buck, BROOK STREAM LANGUAGE p24</p>
</blockquote>
<p>由于CUDA1.0里面没有Stream的概念, 这里没有直接查到是CUDA的哪一个版本提出了Stream. 所以参考了Steve Rennich在GTC的ppt.</p>
<blockquote>
<p>A sequence of operations that execute in issue-order on the GPU - Steve Rennich, <a href="https://developer.download.nvidia.cn/CUDA/training/StreamsAndConcurrencyWebinar.pdf">CUDA C/C++
Streams and Concurrency</a></p>
</blockquote>
<p>Stream在brook指带的是一系统的数据, 而在cuda里面是的一系列的指令.</p>
<h3 id="kernel">Kernel</h3>
<blockquote>
<p>Brook kernels are special functions, specified by the kernel
keyword, which operate on streams. - Ian Buck, BROOK STREAM LANGUAGE p26</p>
</blockquote>
<p>在Brook里面kernel指的是处理stream的函数. 由于stream是的是一系列的数据, 因此kernel也就是处理data的函数.</p>
<blockquote>
<p>More precisely, a portion of an application that is executed many times, but
independently on different data, can be isolated into a function that is executed on
the device as many different threads. To that effect, such a function is compiled to
the instruction set of the device and the resulting program, called a <em>kernel</em>  - Nvidia CUDA Compute Unified Device Architecture Programming Guide, version1.0, p7</p>
</blockquote>
<p>可以看到kernel在Brook与CUDA里面的概念是一致的. 都表示为处理数据的函数.</p>
<h3 id="reductions">Reductions</h3>
<blockquote>
<p>While kernels provide a mechanism for applying a function to a set of data, reductions
provide a data-parallel method for calculating a single value from a set of records. - Ian Buck, BROOK STREAM LANGUAGE p30</p>
</blockquote>
<p>这里需要附上原文的代码才能更详细的说明:</p>
<pre data-lang="c" style="background-color:#eff1f5;color:#4f5b66;" class="language-c "><code class="language-c" data-lang="c"><span style="color:#a7adba;">// Task: Compute the sum of all the elements of a sum (a, r);
</span><span>
</span><span style="color:#a7adba;">// Brook
</span><span>reduce </span><span style="color:#8fa1b3;">sum </span><span>(</span><span style="color:#b48ead;">float</span><span> a&lt;&gt;, reduce </span><span style="color:#b48ead;">float</span><span> r&lt;&gt;) {
</span><span>    r += c;
</span><span>}
</span><span style="color:#b48ead;">float</span><span> r;
</span><span style="color:#b48ead;">float</span><span> a&lt;</span><span style="color:#d08770;">100</span><span>&gt;;
</span><span>
</span><span style="color:#a7adba;">// Equivalant C code:
</span><span>r = a[</span><span style="color:#d08770;">0</span><span>];
</span><span style="color:#b48ead;">for </span><span>(i=</span><span style="color:#d08770;">1</span><span>; i&lt;</span><span style="color:#d08770;">100</span><span>; i++)
</span><span>    r += a[i];
</span></code></pre>
<p>Reduction指的是将从一组数据中计算单个值的任务抽离出来, 很奇怪的一点是明明这里的操作也可以归为kernel的范畴, 但是Ian却单独抽里开. 说明Brook对Reducitons的实现与kernel是不同的. 而从语法上来看, 这段代码与CUDA调用kernel function几乎是一致的. 在CUDA中没有单独的概念, 说明这一操作已经在CUDA统一了.</p>
<p>到此为止, 我们已经了解了Brook里面提出的主要概念并且与CUDA里面的概念做了对比. 了解了Brook与CUDA之间的关系. 尽管Ian在自己的博士论文里面论述了Streaming Processing相比于SIMD的优势. 但是CUDA1.0的编程设计仍然是针对于SIMD Processor而不是后来的Streaming Processor. 而从2006年Ian的博士毕业到2007年CUDA1.0的发布. 中间肯定有受限于当时硬件设计的妥协. 说明任何事物发展都有个规律, 从概念提出最后到完全时间的周期往往会比较长. SIMD到SIMT也不是一蹴而就.</p>
<h2 id="wei-shi-yao-cudazhe-yang-she-ji">为什么CUDA这样设计</h2>
<p>回到开头所提出的问题, 为什么CUDA这样设计? 结合Steve Jone在GTC2021的<a href="https://www.nvidia.com/zh-tw/on-demand/session/gtcspring21-s31151/">&quot;How Gpu Computing Works&quot;</a>以及GTC2022的<a href="https://resources.nvidia.com/en-us-summer-of-learning-for-students/gtcspring22-s41487">&quot;How CUDA Programming Works&quot;</a>, 我尝试用尽可能精炼的语言描述下我的理解:</p>
<p>从CUDA的目标上来看, 制约现在程序性能往往不是由于通常概念上的运算性能, 而是由于内存延迟. CUDA的任务就是让程序尽可能的调动GPU的运算单元, 从而隐藏内存延迟. </p>
<p>从CUDA的实现方式上来看, 因为GPU最开始是为加速渲染所设计并且适合执行大量重复的计算, 因此CUDA会有概念层的抽象. 通过将一张图片或者一次渲染做拆分成grid, 而每个grid会有多个blocks作为操作单元, 每个block的数据可以由多个thread来共享数据. 每个thread是最小操作单元. 通过这样划分, 实现了<a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads">SIMT(Single Instruction Multiple Threads)</a>. 从而隐藏内存延迟.</p>
<p>下面这张图对于理解CUDA的概念非常有帮助:
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/gtc-cuda-concepts.png" alt="cuda-concepts" /></p>
<h2 id="zong-jie">总结</h2>
<p>CUDA的发明者Ian Buck在读博士的时候(2006年左右), 当时已经有专门针对GPU的编程语言, 但是这些特点的语言的特定都是针对于图形学里的shader设计的, 因此不具有通用性. 因此CUDA的意义在于使用流式处理(stream processsing)去解决图形处理器通用计算(General-purpose computing on graphics processing units)软件层抽象的问题.</p>
<h2 id="can-kao-zi-liao">参考资料</h2>
<ul>
<li>How Gpu Computing Works - Steve Jones </li>
<li>How CUDA Programming Works - Steve Jones</li>
<li>BROOK STREAM LANGUAGE - Ian Buck</li>
<li>Brook for GPUs: Stream Computing on Graphics Hardware - Ian Buck</li>
<li>Nvidia CUDA Compute Unified Device Architecture Programming Guide, version1.0</li>
<li>Wikipedia </li>
</ul>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年六月五日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog8&#x2F;" class="post-title-link">LLM OS的曙光</a></div>
        <div class="post-content">
            
                <p>在今天的OpenAI的春季发布会, OpenAI发布了<a href="https://openai.com/index/hello-gpt-4o/">gpt-4o</a>. 虽然前几代模型也支持语音功能, 但是在gpt-4o中, 其中任意打断的能力和更快的响应速度, 实在是难以让人区分出是机器人还是人类. 但是现在每次对话都需要重新用语音prompt, 来让模型产生预期的“性格”. 所以显而易见的, 不久后的GPT商店会上架不同“性格”的AI. 以及对自定义AI的支持.</p>
<p>语音和图像的联动是另一项gpt-4o所突出展示出的能力. 用语音交互的方式来让AI分析实时的摄像头数据. 并且在pc端通过桌面端应用还具备<a href="https://www.youtube.com/watch?v=ZJbu3NEPJN0">voice copliot</a>的能力. 已经能让生产力提高到下一个层次. 很明显在未来能够 <u>清晰的表达想法和理解问题</u> 的能力比单纯的技能更加重要. 其实这已经是LLM OS的一个很大的突破了.</p>
<p>随着OpenAI开放更加便宜的AI, OpenAI的影响力肯定会进一步扩大. 他们所能获取的数据集肯定越来越多. 假如世界模型是一个向量f(x), 根据<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal approximation theorem</a>. 因此世界模型会被拟合, 假如未来的模型架构迭代, 比如使用<a href="https://arxiv.org/pdf/2404.19756">Kolmogorov–Arnold Networks</a>. 实际上变的也是拟合的基函数, 核心思想都是一致. 也是对世界模型的拟合. 因此, AI最终一定会解决现有的世界模型 f(x) 的一切问题. 但是以这种方式的AI不具备创造力.</p>
<p>如何让AI具备创造力? 这篇文章<a href="https://arxiv.org/pdf/2002.06177">The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence</a>的作者就很好的分析了这个问题. <a href="https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/">现有的AI始终受限于数据集</a>. AGI需要有对知识的归纳和演化的能力.以下是原文链接.</p>
<blockquote>
<p>a reasoning system that can leverage large-scale background knowledge efficiently, even when available information is incomplete is a prerequisite to robustness. - p40</p>
</blockquote>
<p>因此在未来的AGI形态，会有一个底座的知识语料, 在这个底座的知识语料当中, AI有知识关联, 归纳, 推理能力. 当AI不断的从世界模型提炼信息的时候, 也会对这个底层的知识语料进行加工. 类比于人类来说的话, 人在不断接触外部信息源的时候也会不断迭代自己的世界观(或者说每个人的人生哲学). 而当人遇到未接触的问题时, 在没有外部信息源的接触下, 也会依赖于自己的世界观进行加工和理解.</p>
<p>或许未来出现一种形态, 在神经网络(知识的拟合)里面嵌套专家系统(知识的推理). 能够让AI真正具有创造力.</p>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年五月十四日

            
        </div>
    </article>
    

</section>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <script>
        var Home = location.href,
        xhr,
        xhrUrl = '';
    
        var Diaspora = {
            L: function(url, f, err) {
                if (url == xhrUrl) {
                    return false;
                }
                xhrUrl = url;
                if (xhr) {
                    xhr.abort();
                }
                xhr = $.ajax({
                    type: 'GET',
                    url: url,
                    timeout: 10000,
                    success: function(data) {
                        f(data);
                        xhrUrl = '';
                    },
                    error: function(a, b, c) {
                        if (b == 'abort') {
                            err && err()
                        } else {
                            window.location.href = url;
                        }
                        xhrUrl = '';
                    }
                });
            },
            loading: function() {
                var w = window.innerWidth;
                var css = '<style class="loaderstyle" id="loaderstyle'+ w +'">'+
                    '@-moz-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '@-webkit-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '.loader'+ w +'{-webkit-animation:loader'+ w +' 3s linear infinite;-moz-animation:loader'+ w +' 3s linear infinite;}'+
                    '</style>';
                $('.loaderstyle').remove()
                $('head').append(css)
                $('#loader').removeClass().addClass('loader'+ w).show()
            },
            loaded: function() {
                $('#loader').removeClass().hide()
            }
        };
    
        $(function() {
            $('body').on('click', function(e) {
                var tag = $(e.target).attr('class') || '',
                    rel = $(e.target).attr('rel') || '';
                if (!tag && !rel) return;
                switch (true) {
                    // next page
                    case (tag.indexOf('more') != -1):
                        tag = $('.more');
                        if (tag.data('status') == 'loading') {
                            return false
                        }
                        var num = parseInt(tag.data('page')) || 1;
                        if (num == 1) {
                            tag.data('page', 1)
                        }
                        tag.html("旧文").data('status', 'loading')
                        Diaspora.loading()
                        Diaspora.L(tag.attr('href'), function(data) {
                            tag.hide();
                            $('.license').hide();
                            var link = $(data).find('.more').attr('href');
                            if (link) {
                                tag.attr('href', link).html("旧文").data('status', 'loaded')
                                tag.data('page', parseInt(tag.data('page')) + 1)
                            }

                            var tempScrollTop = $(window).scrollTop();
                            $('body').append($(data).find('.posts'))
                            $(window).scrollTop(tempScrollTop + 100);
                            Diaspora.loaded()
                            //$('html,body').animate({ scrollTop: tempScrollTop + 400 }, 500);
                            if (link !== '/' && link != '') {
                                $('body').append($(data).find('.page-nav'))
                            }
                        }, function() {
                            tag.html("旧文").data('status', 'loaded')
                        })
                        return false;
                        break;
                    default:
                        return true;
                        break;
                }
            });
        })
    </script>
    
    <nav class="page-nav">
      <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;page&#x2F;8&#x2F;" class="more">旧文</a>
    </nav>


    </main>
    
    <p class="license"></p>
    
</body>
</html>
