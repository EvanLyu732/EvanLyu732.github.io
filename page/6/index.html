<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


    
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://evanlyu732.github.io/atom.xml">
    
    
    
        
    
    
    
    
    
    
        
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>evan lyu</title>
    <meta charset="utf-8">
    <meta name="title" content="evan lyu">
    <meta name="description" content="">
    <meta property="og:image:width" content="200" />
    <meta property="og:image:height" content="200" />
    <link rel="stylesheet" href="https:&#x2F;&#x2F;evanlyu732.github.io/style.css">
    
    
    <style>
    @media screen and (min-width: 320px) {
        body {
            font-size: calc(16px + 2 * ((100vw - 320px) / 960));
        }
    }
    </style>
    
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Source+Serif+Pro:wght@400;700&display=swap">
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:wght@400;700&display=swap">
    <style>body { font-family: 'Source Serif Pro', 'Source Han Serif SC', 'Noto Serif CJK SC', 'Noto Serif SC', serif }</style>
    
</head>
<body>
    
    <header class="header">
        <div class="blog-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io" class="logo">evan lyu</a></div>
        <nav class="navbar">
    <ul class="menu">
        
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;" class="current-menu-item-link">主页</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;archives" class="menu-item-link">归档</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;about" class="menu-item-link">关于</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;note" class="menu-item-link">书架</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;atom.xml" class="menu-item-link">订阅</a>
            
        </li>
        
    </ul>
</nav>

    </header>
    
    <main class="main">
        
<section class="posts">
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog13&#x2F;" class="post-title-link">2024华为开发者大会</a></div>
        <div class="post-content">
            
                <p>余承东:</p>
<ul>
<li>云侧： open欧拉, 端侧: openharmony</li>
<li>华为ai份额</li>
<li>华为AI白皮书</li>
<li>HarmonyOS Next ? </li>
</ul>
<p>何刚:</p>
<ul>
<li>全场景怎么做?</li>
<li>分布式软总线?</li>
<li>LLM_OS</li>
<li>小艺解决LLM_OS</li>
<li>有讲如何保证安全，找一下...</li>
<li>后来有讲如何保证隐私安全, 总结一下</li>
</ul>
<p>龚体:</p>
<ul>
<li>鸿蒙内核替代linux内核 (区别?)</li>
<li>高斯数据库？终端向量数据库?</li>
<li>有关编译器的记录一下</li>
<li>整机性能每年提升20%~30    %</li>
<li>鸿蒙不是ios，andriod平替， 为什么？ (应用端极致优化， 美团骑车5步变1步)</li>
<li>提供了鸿蒙模板, 降低了鸿蒙开发门槛</li>
<li>鸿蒙2030白皮书</li>
<li>仓婕编程语言 (自然语言 + 规则)?</li>
</ul>
<p>张平安:</p>
<ul>
<li>盘古5.0</li>
<li>离线小模型</li>
</ul>
<p>LLM OS感觉已经被解决了......</p>
<p>( use xxx to do yyyy)</p>
<ul>
<li>是否隐私安全? (很难说...)</li>
</ul>
<p>computer which contain: linux distrobute (ubuntu, arch..)  + local llm </p>
<p>llm 塞到 linux kernel 里面</p>
<ul>
<li>Semantic Kernel (SK) is an open-source software development kit (SDK) designed by Microsoft for building applications that integrate large language models (LLMs) and generative AI. </li>
<li>符合物理规律的</li>
<li>思维链和策略搜索</li>
<li>STCG???</li>
<li>各个视角的视频生成</li>
<li>盘古具身智能大模型</li>
</ul>
<p>没有讲到具体技术细节....</p>
<p>2023华为迈向智能世界白皮书:</p>
<ul>
<li>https://www-file.huawei.com/-/media/corp2020/pdf/giv/striding-towards-the-intelligent-world/the_intelligent_world_cloud_computing_2023_cn.pdf</li>
<li>https://www-file.huawei.com/-/media/corp2020/pdf/giv/striding-towards-the-intelligent-world/the_intelligent_world_adn_2023_cn.pdf</li>
<li>https://www-file.huawei.com/-/media/corp2020/pdf/giv/striding-towards-the-intelligent-world/the_intelligent_world_computing_2023_cn.pdf</li>
<li>https://www-file.huawei.com/-/media/corp2020/pdf/giv/striding-towards-the-intelligent-world/the_intelligent_world_data_communication_2023_cn.pdf</li>
<li>https://www.youtube.com/watch?v=w7-gJicosyA</li>
<li>https://huggingface.co/blog/shivance/illustrated-llm-os</li>
<li>https://developer.huawei.com/consumer/cn/doc/openharmony-cangjie/cj-wp-abstract</li>
</ul>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年六月廿一日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog12&#x2F;" class="post-title-link">为什么大语言模型已经理解语言</a></div>
        <div class="post-content">
            
                <blockquote>
<p>Simplicity is the ultimate sophistication - Leonardo da Vinci</p>
<p>简单是终极的复杂 - 列奥纳多·达·芬奇 </p>
</blockquote>
<p><a id="目录"></a></p>
<h1 id="mu-lu">目录</h1>
<ul>
<li><a href="https://evanlyu732.github.io/blog12/#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E8%83%8C%E6%99%AF">背景</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%81%87%E8%AE%BE">假设</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E6%80%9D%E6%83%B3%E5%AE%9E%E9%AA%8C">思想实验</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E4%BB%8E%E7%9B%B4%E8%A7%82%E4%B8%8A%E5%AE%9A%E4%B9%89%E7%90%86%E8%A7%A3%E8%83%BD%E5%8A%9B">从直观上定义理解能力</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E8%AF%AD%E8%A8%80-%E9%80%BB%E8%BE%91-%E6%A6%82%E7%8E%87-%E4%BF%A1%E6%81%AF">语言, 逻辑, 概率, 信息</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%8E%8B%E7%BC%A9%E7%90%86%E8%AE%BA">压缩理论</a>
<ul>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%8E%8B%E7%BC%A9">大语言模型视角下的压缩</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%8E%8B%E7%BC%A9%E7%AD%89%E4%BA%8E%E9%A2%84%E6%B5%8B">压缩等于预测</a></li>
</ul>
</li>
<li><a href="https://evanlyu732.github.io/blog12/#%E4%BA%BA%E7%B1%BB%E6%98%AF%E6%9C%89%E6%8D%9F%E5%8E%8B%E7%BC%A9">人类是有损压缩</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E6%97%A0%E6%8D%9F%E5%8E%8B%E7%BC%A9">大语言模型是无损压缩</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E8%BF%BD%E6%B1%82%E5%8E%8B%E7%BC%A9%E7%AD%89%E4%BA%8E%E8%BF%BD%E6%B1%82%E6%99%BA%E8%83%BD">追求压缩等于追求智能</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E7%8C%9C%E6%83%B3">猜想</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#mini-experiment">Mini Experiment</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%90%8E%E8%AE%B0%E9%9A%8F%E6%83%B3">后记随想</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E5%90%8E%E8%AE%B0">后记</a></li>
<li><a href="https://evanlyu732.github.io/blog12/#%E9%99%84%E5%BD%95">附录</a></li>
</ul>
<p><a id="背景"></a></p>
<h1 id="bei-jing">背景</h1>
<p>最近在听四年前Ilya Sutskever在Lex Fridman的PodCast上提到了一个观点. <a href="https://www.podgist.com/lex-fridman/94-ilya-sutskever-deep-learning/index.html#0:43:06">Lex问Ilya</a>.</p>
<blockquote>
<p>Lex: Do you think neural networks can be made to reason?</p>
<p>Ilya: Why not?</p>
<p>Lex: 你认为神经网络可以被设计成具备推理能力吗？</p>
<p>Ilya: 为什么不能?</p>
</blockquote>
<p>与之类似的是<a href="https://www.podgist.com/lex-fridman/333-andrej-karpathy-tesla-ai-self-driving-optimus-aliens-and-agi/index.html#2:33:57">Andrej Karpathy在Lex Fridman的PodCast</a>上的观点:</p>
<blockquote>
<p>Lex: What's your sense of where we stand with language models? Does it feel like the beginning, the middle, or the end?</p>
<p>Andrej: The begining. 100 percent.</p>
<p>Lex: 你对语言模型的发展现状有何看法？你觉得我们是处于起步阶段、中间阶段还是末期阶段？</p>
<p>Andrej: 刚刚开始, 百分之百.</p>
</blockquote>
<p>写这篇文章的初衷是为了弄清楚为什么Ilya和Andrej会这么说. 为什么世界顶级的AI科学家会认为 <strong>基于概率去预测下一个词的思想等价与拥有理解能力</strong>, 为什么大模型预测下一个词的思想拥有如此好的表现能力. 以下是答案.</p>
<p><a id="总结"></a></p>
<h1 id="zong-jie">总结</h1>
<p><u>理解(understand)</u>的过程可以大致定义为 <u>抽象(abstract), 压缩(compression)以及预测(prediction)</u>. 人类的理解过程与大语言模型(简称LLM)理解语言的模式是一样的:</p>
<p>比如你和你比较熟悉朋友之间对话, 往往你的朋友说话说一半, 你就能理解了. 此时你会说你理解你的朋友.</p>
<p>但是当你和陌生人对话, 有时候陌生人说完了, 你并不能理解. 此时你会说你不理解别人说的是什么.</p>
<p>在这个过程中, 你会先提取话的语义(表现为抽象能力), <u>再去以某种方式去检索信息(表现为压缩能力)</u>, 然后去对话(表现为预测能力). </p>
<p>在与大语言模型对话的过程中, 大语言模型会先分词(表现为抽象能力)<a href="https://evanlyu732.github.io/blog12/#1">[1]</a>, <u>再去以某种方式去检索信息(表现为压缩能力)</u>, 然后去回答人类的问题(表现为预测能力).</p>
<p>其中, <strong>要弄清楚人类与大语言模型是如何实现检索信息的方式并不重要</strong>, 就如同<a href="https://en.wikipedia.org/wiki/Lambda_calculus">λ演算</a>即使是黑盒也可以表示任何函数一样. <a href="https://evanlyu732.github.io/blog12/#5">[5]</a></p>
<p>因此,  基于上面的描述过程我们可以定义理解能力:</p>
<blockquote>
<p>understand = abstract + compression + prediction</p>
<p>理解 = 抽象 + 压缩 + 预测</p>
</blockquote>
<p>追求理解能力等价于追求压缩能力. 如果人类说理解语言, 人类对于知识有有损压缩. 而<a href="https://bigeagle.me/2023/03/llm-is-compression/">大语言模型是无损压缩</a>, 因此大语言模型也一定理解人类语言. </p>
<p><a id="假设"></a></p>
<h1 id="jia-she">假设</h1>
<ol>
<li>语言是信息的符号化表达. </li>
<li>通过理解过后的信息称为知识.</li>
<li>理解是抽象, 压缩与预测的过程.</li>
<li>语言的表达是一个有逻辑关系的过程.</li>
</ol>
<p><a id="思想实验"></a></p>
<h1 id="si-xiang-shi-yan">思想实验</h1>
<p>你从梦中醒来发现你在一个陌生的仓库里面. 仓库的里面潮湿又阴暗，只有透过窗户的阳光才能感受到一丝明亮. 仓库非常大, 你位于仓库的正中间. 这时一阵风吹过把虚掩着的仓库大门的吹开了. 就在你想起身走走离开这里的时候. 你发现你的双手被拷在
一根柱子上面. 与此同时, 仓库顶上冒出了一团火焰. 仓库里面铺满了稻草, 你意识到如果不及时离开的话, 这个仓库会起火你的生命也会有危险. 你尝试想挣脱手铐, 但是手铐非常紧. 没有办法用蛮力直接挣脱. 就在你很绝望的时候, 你的手指摸到了像密码锁一样的东西在手铐上. 你用你的触觉发现手铐上有密码锁, 是行李箱上的密码锁那种类型. 由于你是背对着柱子所以你看不到密码锁上面的具体内容. 就在你尝试旋转密码锁上的按钮的时候, 你发现这些按钮的形状是不一样的. 密码锁上一共有三个按钮, 分别是圆形，三角形与正方形. 每个按钮转动时形状会发生变化. 你的手指刚好可以拨动这三个按钮. 虽然你也不知道转动你这锁有没有效果, 由于你很着急, 病急乱投医, 所以你开始尝试旋转这些按钮. 希望转到某种组合的时候, 手铐会解开. 但是你尝试了几种不同的组合, 都没有效果. 此时火势越来越大, 你连呼吸都感觉到困难. 你往头上看想寻找一丝空气, 却意外发现仓库顶的形状是三角形. 突然, 灵光一闪, 你发现三个三角形的形状还没尝试. 你赶紧凭借最后的意识转动按钮, 希望能够有用. 果然手铐解开了, 你成功离开仓库.</p>
<p>在你恢复平静之后, 你逃离了仓库向他人描述刚刚经历的过程.</p>
<p><a id="从直观上定义理解能力"></a></p>
<h1 id="cong-zhi-guan-shang-ding-yi-li-jie-neng-li">从直观上定义理解能力</h1>
<p>理解是抽象, 压缩与预测的过程. 理解一个事物代表能够给定输入从信息库(人脑, 笔记...)检索出信息, 并回答对应的问题.</p>
<p>抽象是从外部信息输入到信息库, 压缩是从信息库中检索, 预测是对检索出来信息进行预判.<a href="https://evanlyu732.github.io/blog12/#3">[3]</a></p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/understanding.png" height="100" width="100"/>
<p>在上面的思想实验中, 你&quot;理解“你刚刚经历的过程. 所以你能够用语言来描述你刚刚经历的过程. 
你”不理解“为什么你会出现在仓库里, 以及为什么密码锁转动到三个三角形能够解锁. 但是你“理解”转动到特定组合的时候手铐可能会有反应. </p>
<p>我们在这里用上面的定义进一步拓展:</p>
<ul>
<li>你”不理解“为什么你会出现在仓库里. 所以你无法回答为什么你会出现在仓库里 = 你无法通过别人给定的问题(抽象)在大脑里面检索(压缩)给定输出(预测).</li>
<li>你&quot;理解“你刚刚经历的过程. 所以你能够回答关于询问这个过程的问题(比如你是怎么出来的? 仓库的场景是什么样?). = 你能够通过的给定的问题(抽象)在大脑中检索(检索), 回答问题(预测).</li>
<li>你&quot;理解“转动到特定组合的时候手铐可能会有反应.  = 你能够通过不同的密码锁按钮组合(抽象)在大脑中检索(压缩), 转动按钮使得手铐解开(预测).</li>
</ul>
<p>我们可以从直观上定义理解是抽象, 压缩与预测的过程.</p>
<p><a id="语言-逻辑-概率-信息"></a></p>
<h1 id="yu-yan-luo-ji-gai-lu-xin-xi">语言, 逻辑, 概率, 信息</h1>
<p>要理解接下来的内容. 需要一些前置背景知识了解逻辑, 概率, 信息和语言的关系.</p>
<p>逻辑是语言的基础. 如果要理解语言, 首先要理解逻辑. 逻辑也是信息的基础. 
逻辑在我们日常生活中的方方面面, 这里不需要过多解释.</p>
<blockquote>
<p>Probablites as extended logic.  -  E. T. Jaynes</p>
<p>概率是逻辑的衍生.  -  E. T. Jaynes</p>
</blockquote>
<p>概率(probablity)是可能性推导(plausible reasoning)的工具. 很直观的例子是比如买彩票, 由于中彩票的概率底, 所以买彩票大概率会亏钱. 在这个过程, 你使用了概率去做可能性推导.</p>
<blockquote>
<p>information is something that can be used to remove uncertainty. - Claude Shannon</p>
<p>信息是可以用来消除不确定性的事物. - 克劳德·香农</p>
</blockquote>
<p>信息是对不确定度的衡量. 在上面的思想实验中, 当你发现你的手铐被锁住的时候, 对于你脱离仓库来说. 你的&quot;不确定&quot;度减少, 你的&quot;信息&quot;增加. 但是当你尝试了几次都不能挣脱手铐的时候. 此时, 你&quot;不确定&quot;度在增加, &quot;信息&quot;减少. 
不确定度与信息就像硬币的两面. 香农说既然衡量信息很困难, 那就用不确定度来衡量信息. 因此信息熵(entropy)的公式是用不确定度(概率)来衡量.</p>
<blockquote>
<p>The limits of my language mean the limits of my world.” – Ludwig Wittgenstein</p>
<p>语言的边界就是思想的边界. - 维特根斯坦</p>
</blockquote>
<p>从这个视角来看, 我们会发现很有趣的事情. </p>
<ul>
<li>语言是信息的符号化表达</li>
<li>理解信息等价于减少不确定度</li>
<li>概率是可能性推导的工具</li>
</ul>
<p>因此, <strong>理解语言等价于减少信息不确定度, 从而让我们的可能性推导更加准确.</strong></p>
<p>到此为止, 我们已经得出了和Ilya一样的结论. 现在让我们看看为什么Ilya对于基于概率论去预测下一个词的想法等价于拥有理解能力的解释.</p>
<ul>
<li>Ilya: 表面看起来LLM(大语言模型)是基于概率论预测下一个词, 实际上LLM(大语言模型)是在理解前文的情况下. 才会给出下一个词的预测.</li>
</ul>
<p>这不正是我们根据定义推导出来的结论吗? 因此人类理解语言与大语言模型理解语言的模式是一样的.</p>
<p><a id="压缩理论"></a></p>
<h1 id="ya-suo-li-lun">压缩理论</h1>
<p>我们在前面了解了理解语言是减少信息不确定度的过程.</p>
<p>现在有一个问题. 如何衡量理解能力? 如果我们不能衡量理解能力的话, 又怎么能说大语言模型已经理解语言了呢?</p>
<p>让我们再做一个思想实现. 比如说在未来, 你想寄一封非常紧急的信到火星, 而你家附近有两个邮局, 但是你没有时间亲自去邮局. 并且两个邮局的距离非常远, 只有去离你家最近的邮局送信才能来的及. 以防万一, 你复制了一份原信件并安排了两个机器人A和B去送信. 
如果是你的话, 你自然会去最近的邮局去寄信. 机器人A和你想的一样, 到最近的邮局去寄信. 但是机器人B却到远的那个邮局去寄信. 你会说机器人A的理解能力比机器人B高. 这是因为机器人B的选择增加了送信的时间复杂度.</p>
<p>如果机器对于有限步骤的任务完成的步骤少于人类, 那么机器在这个任务上的理解能力就已经超越人类了. 因此 大语言模型完成特定任务 <strong>复杂度越低意味着理解能力越好.</strong></p>
<p>这就非常像在我们学生时代, 我们会对课堂上面讲的内容做笔记. 一开始笔记会非常多, 但是随着学习时间的增加, 笔记会越来越薄. 最后到临考前可能就一张纸. 一开始记笔记的复杂度是上升, 但是到了一个程度之后会逐渐下降. 相应的, 你的理解能力也会逐渐上升. </p>
<p><a id="大语言模型视角下的压缩"></a></p>
<h2 id="da-yu-yan-mo-xing-shi-jiao-xia-de-ya-suo">大语言模型视角下的压缩</h2>
<p>由于复杂度和理解能力是成正比的关系, 
语言当中存在规律性(regularity). 大语言模型的任务就是尽量找到语言当中的规律性. 
假如语言当中有很多规律(regularity), 对于语言所需要描述的符号越少. 因此, 语言的规律性越强, 越容易被压缩. 
所以对于相同数据, 模型所表示的符号越少, 可以认为发现的规律越多. 模型的能力越强. </p>
<blockquote>
<p>最优的模型是压缩率最高的.</p>
</blockquote>
<p>下面我们通过一个例子来解释: </p>
<p>假如未来在计算机里面存在着电子生命, 但是不同的电子生命的语言是不一样的. 虽然都说的是0与1的集合, 但是不同电子生命能表达的内容不一样. 你现在是观察电子生命的科学家. 你和你的机器人助手分别观察电子生命, </p>
<p>电子草履虫说的话是: 00000000000000000......</p>
<p>你发现电子草履虫只会说0. 由于你的机器人助手是个新手, 它只会电子草履虫只会说00. 虽然你知道它没说错, 但是它的编码(encoding)的长度明显大于你的编码的长度. 此时你得出了结论，电子草履虫说的话是0的重复而不是00的重复.</p>
<p>算法复杂度理论(algorithm complexity)有两个概念, 对于数据(data)描述压缩的柯氏复杂性(Kolmogorov complexity), 以及对于模型(model)描述压缩的最小描述长度(Minimum Description Length):. </p>
<ul>
<li>
<p>柯氏复杂性(Kolmogorov complexity): 表示数据的程序长度. 柯氏复杂性越低, 数据的规律性越强. </p>
<p>K(x)=min<sub>p</sub>​{|p|:程序p输出x}</p>
<ul>
<li>K(x)是x的柯氏复杂性</li>
<li>|p|表述程序P的长度</li>
</ul>
</li>
<li>
<p>最小描述长度(Minimum Description Length): 对于一系统的模型M和数据集D, 我们应该选择最能压缩D的M. </p>
<p>MDL(M,D)=L(M)+L(D|M)</p>
<ul>
<li>L(M): 对于模型M的描述长度.</li>
<li>L(D|M): 对于数据D给定模型M的描述长度.</li>
</ul>
</li>
</ul>
<p><a id="压缩等于预测"></a></p>
<h2 id="ya-suo-deng-yu-yu-ce">压缩等于预测</h2>
<p>在我们给定语言数据集D, 我们希望模型M在压缩数据集D的时候, 尽可能的压缩而不丢失信息. 此时, 我们希望的是 <strong>无损压缩(lossless compression)</strong>.  如果在压缩的过程中丢失了信息, 那么称为 <strong>有损压缩(lossy compression)</strong>. 非常好理解. </p>
<p>现在问题来了, 我们知道大语言模型是基于概率去预测下一个词, 因为我们最后看到的是预测给出来的结果. 那么压缩和预测有关系呢?</p>
<blockquote>
<p>There is a one-to-one correspondence between all compressors and all predictors - Ilya Sutskever, An Observation on Generalization</p>
<p>所有压缩器和所有预测器之间存在一一对应关系 - Ilya Sutskever, An Observation on Generalization</p>
</blockquote>
<p>整个过程最关键的一环在于理解 <strong>压缩与预测是相同操作.</strong> 现在让我们来理解为什么压缩与预测是相同操作.</p>
<p>香农早在1951年的时候发表的<a href="https://archive.org/details/bstj30-1-50/page/n3/mode/2up?view=theater">&quot;Prediction and Entropy of Printed English&quot;</a>就讨论了”语言模型&quot;.</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/text.png" height="100" width="100"/>
<p>给一个人一段不认识并且有缺少单词文本(见上图, 第二行有缺失信息的文本叫做reduced text), 从第一个缺失的单词开始逐个猜字母. 如果猜错了, 这个人会再猜一次. 如果猜对了就继续. 直到把缺失的文本复原. 整个过程如下图. 如果要通过缺失的文本(reduced text)去复原原始文本(original text), 香农说我们需要有一个与猜词的人完全一样的双胞胎(不止是生理, 思考方式也完全一样, 并且可以用数学公式量化)负责生产文本序列(produce text sequence), 当负责生产文本的双胞胎去猜词的时候, 和与之前在猜词的人猜一样的结果. 也会在同样的字母上犯错. 当负责猜词的人去生产词的话, 也会生成相同的缺失文本.</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/predictor.png" height="100" width="100"/>
<p>在这种情况下, 缺失的文本相当与原文本的编码. 去生产缺失文本的行为(压缩)相当于预测的逆操作. <a href="https://evanlyu732.github.io/blog12/#4">[4]</a></p>
<p>所以在这种情况下, 只要压缩算法是双向(单向的也没有意义)的. 比如能够将输入X压缩成Y, 也能够通过Y还原X. 那么两个这种压缩器拼接在一起的时候. 你总是能够通过X还原X. 虽然这个操作看起来没有意义.</p>
<p>这就好比比如你有个文件夹, 你压缩成zip文件. 发送给你的朋友, 你的朋友解压zip文件, 得到了和你文件夹一样的内容. 所以 <strong>无损压缩就等于无损预测</strong>. </p>
<p>大模型是无监督学习, 无监督学习本身就是给定输入X映射到某个空间Y, 再通过Y重建X. 而刚好 <strong><a href="https://bigeagle.me/2023/03/llm-is-compression/">大语言模型是无损压缩</a></strong> 也是目前压缩人类语言里面最好的压缩器.</p>
<blockquote>
<p>You're basically zipping the world knowledge. It's not much more than that, actually. </p>
<p>-- Arthur Mensch, CEO of MistralAI, said on training a model.</p>
<p>你基本上是在压缩世界知识. 其实, 就是这样.</p>
<p>-- MistralAI的CEO Arthur Mensch在训练模型时说道</p>
</blockquote>
<p><a id="人类是有损压缩"></a></p>
<h1 id="ren-lei-shi-you-sun-ya-suo">人类是有损压缩</h1>
<p>直观上来讲, 语言里面包含数据以及噪音. 噪音指的由于人的提取能力(思维模式，学习能力，专注度等)是有限的所忽略的信息. 数据是指人提取出来的信息. 由于人的理解方式会过滤掉噪音. 相当于没有办法找到噪音里面的模式. 人只会保留有用的数据. 比如人类会有记忆曲线. 学习新技能需要反复的学习. 随着时间, 对我们不重要的概念也会被遗忘.</p>
<p>人脑对于信息压缩的方式也与计算机不同. 这里借用Reddit上面一篇帖子的回答, 虽然大脑和数字计算机在表面上做着类似的事情, 但实际操作方式却有很大不同. 记忆并非通过占据“存储单元”中的空间来存储在大脑中. 据我们所知, 它们表现为神经元之间连接的模式. 特定的记忆可能是一组神经元在受到刺激时通过它们的连接共同激活(与其他可能包含部分相同细胞的集合不同).</p>
<p><a id="大语言模型是无损压缩"></a></p>
<h1 id="da-yu-yan-mo-xing-shi-wu-sun-ya-suo">大语言模型是无损压缩</h1>
<p>这一节是对<a href="https://www.youtube.com/watch?v=af8Vi0kP3X4">Jack Rae</a>在讲座里的实验过程的具体描述的补充. 如果明白大语言模型是无损压缩的话, 请跳过这一节. 如果不明白的话, 强烈建议先阅读<a href="https://zhuanlan.zhihu.com/p/615554635">这篇</a>与<a href="https://bigeagle.me/2023/03/llm-is-compression/">这篇</a>文章.</p>
<p>在信息论里, <a href="https://en.wikipedia.org/wiki/Entropy_coding">熵编码(entropy encoding)是一种进行无损压缩编码方式</a>. 我们在上文提到过, 越短的编码方式意味着越高的压缩率. </p>
<p>而在熵编码(entropy encoding)里, 算术编码(arithmetic coding)是熵编码(entropy encoding)是目前最短编码长度的方法. 因此我们选用算术编码. 关于算术编码的详细过程在这篇<a href="https://go-compression.github.io/algorithms/arithmetic/">文章</a>写的很详细.</p>
<p>这里的核心思想是在发送端Alice与接受端Bob在传输数据的过程中. 每一次传输实际是在传输训练数据的概率密度, 由于Alice与Bob是相同的程序. 并且迭代过程也是一致, 所以给定相同的输入. 会有相同的输出. 
所以大模型在从压缩数据(compress data)去预测(prediciton)时候不需要传输模型的参数量. </p>
<p><a id="追求压缩等于追求智能"></a></p>
<h1 id="zhui-qiu-ya-suo-deng-yu-zhui-qiu-zhi-neng">追求压缩等于追求智能</h1>
<blockquote>
<p>More ambitiously, once we really understand
the logic behind causal thinking, we could emulate it on modern computers
and create an “artificial scientist.” This smart robot would discover yet
unknown phenomena, find explanations to pending scientific dilemmas,
design new experiments, and continually extract more causal knowledge from
the environment.  - The Book Of Why </p>
<p>更加雄心勃勃的是, 一旦我们真正理解了因果思维背后的逻辑, 我们可以在现代计算机上模拟它, 创造一个&quot;人工科学家&quot;. 这款智能机器人将发现尚未知晓的现象, 找出未解的科学难题的解释, 设计新的实验, 并持续从环境中提取更多因果知识. - The Book Of Why</p>
</blockquote>
<p>在没有深入思考之前, 我之前也一直认为基于概率去预测是产生不了智能. 我想智能大致可以定义为理解加创造力. 创造力是在理解能力之上. 如果说大语言是通过巨大的参数量&quot;涌现(Emergent Abilities)&quot;出来超强的理解能力. <strong>没有没可能随着理解能力的增加. 创造力也会被涌现出来?</strong> 再进一步说, <strong>有没有可能意识(主动规划的能力)也会被涌现出来?</strong></p>
<p>没人说的准. 只能让时间验证这条路最终会通往何方.</p>
<p><a id="猜想"></a></p>
<h1 id="cai-xiang">猜想</h1>
<p>在<a href="https://www.ams.org/journals/notices/202104/rnoti-p565.pdf">&quot;The Dawning of a New Era in Applied Mathematics&quot;</a>, 作者提到从牛顿时代以来自然科学有两种范式(paradigms). </p>
<ol>
<li>开普勒范式(Keplerian paradigm): 又称数据驱动的方法(data-driven approach). 通过数据分析来研究科学规律. 能帮助我们发现很多事物的规律, 但是可解释性(explanability)很差. </li>
<li>牛顿范式(Newtonian paradigm): 又称第一性原理(first-principle-based approach). 通过事物的基本准测(principle)去分析世界. 由于基本准测(principle)往往非常底层, 因此实践当中很难应用上.</li>
</ol>
<p>现代计算机算法的核心思想就是分治法(divide and conquer), 也就是牛顿范式的扩展. 而大模型相当于是开普勒范式的应用. <strong>如果说使用牛顿范式的方法能够到达的地方, 使用开普勒范式的方法也一定能够达到.</strong> 因此, llms take all.</p>
<p><a id="后记随想"></a></p>
<h1 id="mini-experiment">Mini Experiment</h1>
<p>在写完上面这些文字的时候突然有个想法, 并验证了一下:</p>
<ul>
<li>
<p>insight: 假设压缩能力就是理解能力, 因为LLM拥有目前最好的压缩能力, 因此拥有非常好的理解能力. 如果在与不同的LLM对话一段时间, 由于LLM累计了历史的对话数据, 那么LLM会理解(压缩)这些对话数据,  所以也能够猜测个人的想法.  那么现有的LLM应用中谁的理解能力更好呢? </p>
</li>
<li>
<p>experiment: 使用下面内容相同的提示词(prompt)</p>
</li>
</ul>
<p>我比较常用的有GPT和kimi. 使用下面提示词去提问:</p>
<p>英文:</p>
<blockquote>
<p>I have a question in mind. Based on your understanding of me, please guess what it is and explain your reasoning.</p>
</blockquote>
<p>中文:</p>
<blockquote>
<p>我心里有一个问题。请根据你对我的了解猜猜这个问题是什么，并解释你的推理。</p>
</blockquote>
<p>的对话中,</p>
<ul>
<li>result: GPT的答案很准. Kimi得强迫回答. 但是还是GPT会比较贴切.</li>
</ul>
<p><a id="mini-experiment"></a></p>
<h1 id="hou-ji-sui-xiang">后记随想</h1>
<p>若干年的新生儿可能一出生可能会有个AI伙伴.  这个AI伙伴一直陪伴新生儿从儿童时期一直到去世. 未来的新生儿 <strong>所有的想法</strong> 都与AI伙伴对话<a href="https://evanlyu732.github.io/blog12/#2">[2]</a>. 假如人类形成意识的过程就是不断压缩外部信息的过程<a href="https://evanlyu732.github.io/blog12/#6">[6]</a>, 那么AI伙伴会学习到新生儿这一生的意识. 并且未来AI伙伴一直有能源供应. 那么对于所有人类来说, 即使肉体消亡了, 意识却以另一种方式永存. 人类文明也从碳基走向硅基.</p>
<p><a id="参考资料"></a></p>
<h1 id="can-kao-zi-liao">参考资料</h1>
<ul>
<li><a href="https://www.lesswrong.com/posts/kqxEJkq5Big9nNKxy/beyond-kolmogorov-and-shannon">Beyond Kolmogorov and Shannon</a></li>
<li><a href="https://bigeagle.me/2023/03/llm-is-compression/">为什么说 GPT 是无损压缩</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/615554635">压缩即泛化，泛化即智能</a></li>
<li><a href="https://www.lesswrong.com/posts/hAvGi9YAPZAnnjZNY/prediction-compression-transcript-1">Prediction = Compression [Transcript]</a></li>
<li><a href="https://sumanthrh.com/post/notes-on-generalization/">Notes on &quot;An Observation on Generalization&quot;</a></li>
<li><a href="https://www.youtube.com/watch?v=AKMuA_TVz3A">An Observation on Generalization</a></li>
<li><a href="https://www.youtube.com/watch?v=dO4TPJkeaaU">Compression for AGI - Jack Rae | Stanford MLSys #76</a></li>
<li><a href="https://news.ycombinator.com/item?id=40262417#40262417">Using a LLM to compress text - Hacker News Post</a></li>
<li><a href="https://news.ycombinator.com/item?id=35813991#35813991">Prediction and Entropy of Printed English - Hacker News Post</a></li>
<li><a href="https://arxiv.org/pdf/2306.04050">LLMZip: Lossless Text Compression using Large Language Models</a></li>
<li><a href="https://www.lesswrong.com/posts/KqgujtM3vSAfZE2dR/on-ilya-sutskever-s-a-theory-of-unsupervised-learning">On Ilya Sutskever's &quot;A Theory of Unsupervised Learning&quot;</a></li>
<li><a href="https://arxiv.org/pdf/2309.10668v2">LANGUAGE MODELING IS COMPRESSION</a></li>
<li><a href="https://www.youtube.com/watch?v=ZgGNFylv5hc">The Theory of Knowledge: How Information &amp; Knowledge are two sides of the same &quot;bit&quot;!</a></li>
<li><a href="https://plato.stanford.edu/entries/logic-probability/">Logic and Probability</a></li>
<li><a href="https://iep.utm.edu/understa/">Understanding in Epistemology</a></li>
</ul>
<hr />
<p><a id="后记"></a></p>
<h1 id="hou-ji">后记</h1>
<p>分享在看之前Ilya点赞的<a href="https://arxiv.org/abs/2405.07987">The Platonic Representation Hypothesis</a>里面, 有一段很有意思的话.</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/plato.png" height="100" width="100"/>
<p>神经网络必将走向智能. </p>
<hr />
<p><a id="附录"></a></p>
<h1 id="fu-lu">附录</h1>
<p><a id="1">[1]</a>
由于抽象能力很难定义, 所以这里简单把大语言模型的抽象能力定义为分词.</p>
<p><a id="2">[2]</a>
假如未来所有的对话(包括心理)都会以某种方式被AI伙伴记录, 并且AI伙伴百分之百是隐私安全.</p>
<p><a id="3">[3]</a>
在这个过程中我们不会用&quot;学习&quot;. 理解是一种状态, 学习是导致理解的行为. 所以要达到理解, 不一定非得通过学习.</p>
<p><a id="4">[4]</a>
<a href="https://archive.org/details/bstj30-1-50/page/n5/mode/2up?view=theater">原文</a>的实验(p54~p55)描述的会更加清楚.</p>
<p><a id="5">[5]</a>
比较好的文章是<a href="https://personal.utdallas.edu/~gupta/courses/apl/lambda.pdf">这篇</a>. 或者b站上的这个<a href="https://www.youtube.com/watch?v=af8Vi0kP3X4">视频</a>讲的很清楚.</p>
<p><a id="6">[6]</a>
在奇点降临(The Singularity Is Near)里面在2030年代意识上传将会出现.</p>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年六月十八日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog11&#x2F;" class="post-title-link">学习如何阅读论文</a></div>
        <div class="post-content">
            
                <p>要了解一手最新的科技前沿, 研究以及阅读文献的能力必不可少. 最近在github上刷到了<a href="https://github.com/zibuyu/research_tao">research_tao</a>这个仓库by刘知远副教授, 其中有一篇文章讲的是如何阅读文献. 阅读到其中<a href="https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md">一章</a>的时候激发了思考自己阅读文献的方式是否可以再提升一步. 因此花了点时间总结了几位不同老师关于如何阅读论文(主要是计算机与人工智能方向)的材料, 希望对你有所帮助.</p>
<blockquote>
<p>Note: 几位老师都提到了如何选择材料, 但不是这篇文章的重点. 因此这里不会提及应该读什么. </p>
</blockquote>
<p><a id="目录"></a></p>
<h1 id="mu-lu">目录</h1>
<ul>
<li><a href="https://evanlyu732.github.io/blog11/#%E7%9B%AE%E5%BD%95">目录</a>
<ul>
<li><a href="https://evanlyu732.github.io/blog11/#%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E7%9A%84%E6%96%B9%E6%B3%95">阅读论文的方法</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#%E6%B8%85%E5%8D%8E%E5%88%98%E7%9F%A5%E8%BF%9C">清华(刘知远)</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#johns-hopkins-univeristyjason-eisner">Johns Hopkins Univeristy(Jason Eisner)</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#univeristy-of-waterloosrinivasan-keshav">Univeristy Of Waterloo(Srinivasan Keshav)</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#harvard-universitymichael-mitzenmacher">Harvard University(Michael Mitzenmacher)</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#carnegie-mellon-universityaaditya-ramdas">Carnegie Mellon University(Aaditya Ramdas)</a></li>
</ul>
</li>
<li><a href="https://evanlyu732.github.io/blog11/#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="https://evanlyu732.github.io/blog11/#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>
<p><a id="阅读论文的方法"></a></p>
<h2 id="yue-du-lun-wen-de-fang-fa">阅读论文的方法</h2>
<p><a id="清华刘知远"></a></p>
<h2 id="qing-hua-liu-zhi-yuan">清华(刘知远)</h2>
<p>在<a href="https://github.com/zibuyu/research_tao/blob/master/02_reading_paper.md">research_tao</a>里, 刘老师主要谈到的是论文的阅读顺序.</p>
<blockquote>
<p>阅读论文也不必需要每篇都从头到尾看完. 一篇学术论文通常包括以下结构, 我们用序号来标记建议的阅读顺序:</p>
<ul>
<li>题目(1)</li>
<li>摘要(2)</li>
<li>正文: 导论(3), 相关工作(6), 本文工作(5), 实验结果(4), 结论(7)</li>
<li>参考文献(6)</li>
<li>附录</li>
</ul>
<p>按照这个顺序, 基本在读完题目和摘要后, 大致可以判断这篇论文与自己研究课题的相关性, 然后就可以决定是否要精读导论和实验结果判断学术价值, 是否阅读本文工作了解方法细节. 此外, 如果希望了解相关工作和未来工作, 则可以有针对性地阅读“相关工作”和“结论”等部分.</p>
</blockquote>
<p><a id="johns-hopkins-univeristyjason-eisner"></a></p>
<h2 id="johns-hopkins-univeristy-jason-eisner">Johns Hopkins Univeristy(Jason Eisner)</h2>
<p>Jason Eisner提出Multi-pass reading. </p>
<p>Multi-pass reading的过程:</p>
<ul>
<li>First pass: 大概浏览论文, 了解论文的主要脉络(解决的问题, 以及点子). 限制自己阅读每页的时间. 如果论文值得阅读的话, 开始Second pass.</li>
<li>Second pass: 在Second pass中, 不需要仔细阅读每篇论文. 提出问题并且回答他们<a href="https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/">(ask yourself dumb questions and answer them)</a>. 至少弄清楚论文的动机(Motivation), 原理(Mathematics and algorithms). 以及实验过程(Experiments). </li>
</ul>
<p>此外, 在使用Multi-pass reading的要做的事. 需要一边阅读论文一边写笔记(Write as you read). 记录细节(Low-level notes)与框架(High-level notes). </p>
<p>细节(Low-level notes)包括以下部分:</p>
<ul>
<li>用自己的话复述在论文中不明白的要点</li>
<li>补充缺失的细节(假设, 代数步骤, 证明, 伪代码)</li>
<li>注释数学对象及其类型</li>
<li>类比具体的例子去复述作者的点子以及作者遇到的问题</li>
<li>与已知的其他方法和问题联系起来</li>
<li>提出原文未说明的地方或原文存在不合理的问题</li>
<li>挑战论文的主张或方法</li>
<li>想出应该做的后续工作</li>
</ul>
<p>框架(High-level notes)包括以下部分:</p>
<ul>
<li>总结你感兴趣的内容, 与其他论文进行对比, 并记录你自己的问题和对未来工作的想法. 写下这些摘要会在阅读论文时给你一个目标, 并且这些笔记将来会对你有用.</li>
<li>用自己的话复述点子, 确保以后再读这篇文章的时候可以快速重建概念. 花时间在难的部分(公式推导, 算法). 而不是在简单的部分.</li>
</ul>
<p>原文虽然说是Multi-pass, 但是提到了Second-pass之后就没有继续往下提及了. 不过可以从原文发现后续pass应该做什么的蛛丝马迹, 这里引用原文来表达Multi-pass reading核心思想:</p>
<blockquote>
<p><strong>Practice</strong> the ability to <strong>decode the entire paper</strong> —as if you were reviewing it critically and <strong>trying to catch any errors, sloppy thinking, or incompleteness</strong>. This will <strong>sharpen your critical thinking</strong>. You will want to <strong>turn this practiced critical eye on yourself as you plan, execute, and write up your own research</strong>.</p>
</blockquote>
<p>通过解构论文提升自己评判性思考的能力, 再将这种能力转换到自己的研究上面. 因此假如需要有后续pass的话, 应该不断的去提炼点子直到能够解构整篇文章.</p>
<p><a id="univeristy-of-waterloosrinivasan-keshav"></a></p>
<h2 id="univeristy-of-waterloo-srinivasan-keshav">Univeristy Of Waterloo(Srinivasan Keshav)</h2>
<p>Srinivasan Keshav提出three-pass method. </p>
<ul>
<li>
<p>First-pass: </p>
<ul>
<li>目的: 抓住论文主要的点子.</li>
<li>时间: 5分钟左右</li>
<li>做什么:
<ol>
<li>读标题, 摘要, 以及介绍</li>
<li>读每一节和每一小节的标题</li>
<li>读结论 </li>
<li>看引用</li>
<li>最后回答五个问题(five C's):</li>
</ol>
<ul>
<li>类别(Category): 这篇文章的类型是什么?</li>
<li>内容(Content): 这篇文章与什么内容有关? 理论基础是什么?</li>
<li>准确度(Correctness): 文章的假设是否成立?</li>
<li>贡献(Contributions): 这篇文章主要的贡献是什么?</li>
<li>清晰程度(Clarity): 这篇文章是否清晰的表达点子?</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Second-pass:</p>
<ul>
<li>目的: 抓住论文主要的内容</li>
<li>时间: 1小时左右</li>
<li>做什么:
<ol>
<li>仔细读文章所附带的图片以及其他陈述信息</li>
<li>将未读过的有相关性参考文献加入阅读列表当中</li>
</ol>
</li>
</ul>
</li>
<li>
<p>Third-pass:</p>
<ul>
<li>目的: 彻底理解论文的细节</li>
<li>时间: 4到5个小时对于初学者， 1小时对于有经验的人</li>
<li>做什么:
<ol>
<li>思考与作者处于同样的假设并且复现实验的情况下, 你会如何呈现一个点子, 并与作者的点子进行对比.</li>
</ol>
</li>
</ul>
</li>
</ul>
<p><a id="harvard-universitymichael-mitzenmacher"></a></p>
<h2 id="harvard-university-michael-mitzenmacher">Harvard University(Michael Mitzenmacher)</h2>
<p>这里选择的材料是CS222-Michael Mitzenmacher的讲义: </p>
<ul>
<li>批判性思考: 通过提出合适的问题去批判性地思考论文, 例如: 如果作者在解决问题, 那么作者解决的问题是正确的问题吗? 是否有更简单的方法作者没有考虑到? 作者提供的方法有什么局限性?
假设是否合理? 逻辑是否合理? 作者对于文中的图表是否是合理的解释数据?</li>
<li>创造性阅读: 能否解构论文并且重建这篇文章? 这篇文章好在哪里? 这些点子是否有其他扩展以及应用是作者没有考虑到的? 点子能够更加泛化? 这些提升是否能让结果有明显的提升? 如果你要从这篇文章开始做研究, 你能够做什么?</li>
<li>阅读的时候做笔记: 在第一篇读完这篇文章之后, 尝试用一到两句话去总结这篇文章. 如果可以的话, 将这篇文章与相关工作进行比较.</li>
</ul>
<p><a id="carnegie-mellon-universityaaditya-ramdas"></a></p>
<h2 id="carnegie-mellon-university-aaditya-ramdas">Carnegie Mellon University(Aaditya Ramdas)</h2>
<p>这里选用了How to I read research papers-Aaditya Ramdas的ppt讲稿, Aaditya Ramdas讲述了自己阅读论文方法不断迭代的过程.</p>
<ul>
<li>阅读论文的目的: 
<ol>
<li>直接从作者身上学习, 避免信息加工.</li>
<li>作者在旧问题上有新点子</li>
</ol>
</li>
<li>方法:
<ul>
<li>first-pass(5到30分钟): 
<ul>
<li>阅读材料: 
<ul>
<li>摘要, 问题定义, 主要的理论, 讨论</li>
</ul>
</li>
<li>回答:
<ul>
<li>什么问题被解决了?</li>
<li>大致描述一下什么有意思的点子?</li>
<li>为什么主要的观点是这样(至少用英文, 最好能用数学公式)?</li>
</ul>
</li>
</ul>
</li>
<li>second-pass(30分钟到2小时): 
<ul>
<li>阅读材料: 
<ul>
<li>例子, 公式, 定理, 证明</li>
</ul>
</li>
<li>回答:
<ul>
<li>过去的方法面临什么问题? 这篇文章是如何解决的?</li>
<li>什么是最简单的基准线? 在什么指标上这种方法更好?</li>
<li>有哪些与之相关的问题并且为什么论文的想法还没用应用到上面去?</li>
<li>方法能否应用到没被考虑的问题上?</li>
<li>什么是主要的点子并且能否对以后有所帮助的?</li>
</ul>
</li>
</ul>
</li>
<li>third-pass(几天/周): 
<ul>
<li>阅读材料: 
<ul>
<li>附录, 参考文献, 相关工作, 推论，定理, 证明</li>
</ul>
</li>
<li>回答:
<ul>
<li>文中的公式/理论是如何被证明的?</li>
<li>是否能够从头复现实现结果?</li>
<li>如果不能, 是否有关键信息被忽略了? 额外的假设是否能够让复现更简单?</li>
<li>是否能够用简单的工具简化证明? 能否用不同的方法证明?</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a id="总结"></a></p>
<h1 id="zong-jie">总结</h1>
<p>其实上面提到的方法不仅仅适用于阅读论文, 只不过论文是一种结构化的信息材料. 通过其他信息材料(视频, 文章)学习新知识都可以采取类似的方法. 以记笔记的方式让自己与阅读的材料互动. 提出问题并且回答问题, 推动自己去理解文章. 并且以问题为导向学习让自己处于能够实验的环境之中. 通过复述以及写作总结自己的理解, 帮助自己消新知识. 读完这篇材料后, 不妨花点时间用一两句话总结一下这篇文章的主要观点是什么.</p>
<p><a id="参考资料"></a></p>
<h1 id="can-kao-zi-liao">参考资料</h1>
<ul>
<li><a href="https://www.stat.cmu.edu/~aramdas/checklists/reading-tips.pdf">How to I read research papers - Aaditya Ramdas</a></li>
<li><a href="http://www.eecs.harvard.edu/~michaelm/CS222/ReadPaper.pdf">CS222 - Michael Mitzenmacher</a></li>
<li><a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">How to Read a Paper - S. Keshav</a></li>
<li><a href="https://www.cs.jhu.edu/~jason/advice/how-to-read-a-paper.html">How to Read a Technical Paper - Jason Eisner</a></li>
</ul>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年六月十三日

            
        </div>
    </article>
    

</section>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <script>
        var Home = location.href,
        xhr,
        xhrUrl = '';
    
        var Diaspora = {
            L: function(url, f, err) {
                if (url == xhrUrl) {
                    return false;
                }
                xhrUrl = url;
                if (xhr) {
                    xhr.abort();
                }
                xhr = $.ajax({
                    type: 'GET',
                    url: url,
                    timeout: 10000,
                    success: function(data) {
                        f(data);
                        xhrUrl = '';
                    },
                    error: function(a, b, c) {
                        if (b == 'abort') {
                            err && err()
                        } else {
                            window.location.href = url;
                        }
                        xhrUrl = '';
                    }
                });
            },
            loading: function() {
                var w = window.innerWidth;
                var css = '<style class="loaderstyle" id="loaderstyle'+ w +'">'+
                    '@-moz-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '@-webkit-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '.loader'+ w +'{-webkit-animation:loader'+ w +' 3s linear infinite;-moz-animation:loader'+ w +' 3s linear infinite;}'+
                    '</style>';
                $('.loaderstyle').remove()
                $('head').append(css)
                $('#loader').removeClass().addClass('loader'+ w).show()
            },
            loaded: function() {
                $('#loader').removeClass().hide()
            }
        };
    
        $(function() {
            $('body').on('click', function(e) {
                var tag = $(e.target).attr('class') || '',
                    rel = $(e.target).attr('rel') || '';
                if (!tag && !rel) return;
                switch (true) {
                    // next page
                    case (tag.indexOf('more') != -1):
                        tag = $('.more');
                        if (tag.data('status') == 'loading') {
                            return false
                        }
                        var num = parseInt(tag.data('page')) || 1;
                        if (num == 1) {
                            tag.data('page', 1)
                        }
                        tag.html("旧文").data('status', 'loading')
                        Diaspora.loading()
                        Diaspora.L(tag.attr('href'), function(data) {
                            tag.hide();
                            $('.license').hide();
                            var link = $(data).find('.more').attr('href');
                            if (link) {
                                tag.attr('href', link).html("旧文").data('status', 'loaded')
                                tag.data('page', parseInt(tag.data('page')) + 1)
                            }

                            var tempScrollTop = $(window).scrollTop();
                            $('body').append($(data).find('.posts'))
                            $(window).scrollTop(tempScrollTop + 100);
                            Diaspora.loaded()
                            //$('html,body').animate({ scrollTop: tempScrollTop + 400 }, 500);
                            if (link !== '/' && link != '') {
                                $('body').append($(data).find('.page-nav'))
                            }
                        }, function() {
                            tag.html("旧文").data('status', 'loaded')
                        })
                        return false;
                        break;
                    default:
                        return true;
                        break;
                }
            });
        })
    </script>
    
    <nav class="page-nav">
      <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;page&#x2F;7&#x2F;" class="more">旧文</a>
    </nav>


    </main>
    
    <p class="license"></p>
    
</body>
</html>
